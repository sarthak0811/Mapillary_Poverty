# NOTE: to reproduce the results from our paper, the training _must_ be run using at least 8 GPUs
# NOTE: the `coco_gt` parameter must be filled with the complete path to the [db]/coco/validation.json file generated by
# the scripts/data_preparation/prepare_vistas.py script when pre-processing the dataset

[general]
val_interval = 5
log_interval = 5

[body]
body = resnet50
normalization_mode = syncbn
body_params = {}
bn_frozen = no

[fpn]
extra_scales = 2
out_strides = (4, 8, 16, 32, 64, 128)

[rpn]
anchor_ratios = (0.2, 0.5, 1, 2, 5)
anchor_scale = 2
num_pre_nms_train = 12000
num_post_nms_train = 2000
num_pre_nms_val = 6000
num_post_nms_val = 1000
min_size = 0
fpn_min_level = 0
fpn_levels = 6

[roi]
num_samples = 512
nms_threshold = 0.5
score_threshold = 0.05
max_predictions = 256

[sem]
fpn_min_level = 0
fpn_levels = 4
pooling_size = (64, 64)
ohem = .25

[optimizer]
lr = 0.01
weight_decay = 0.0001
weight_decay_norm = no
momentum = 0.9
nesterov = yes
# obj, bbx, roi_cls, roi_bbx, roi_msk, sem
loss_weights = (1., 1., 1., 1., 1., 1.)

[scheduler]
epochs = 85
type = multistep
update_mode = batch
params = {"gamma": 0.1, "milestones": [144000, 176000]}
burn_in_steps = 200

[dataloader]
shortest_size = 1920
longest_max_size = 2304
train_batch_size = 1
val_batch_size = 1
rgb_mean = (0.41738699, 0.45732192, 0.46886091)
rgb_std = (0.25685097, 0.26509955, 0.29067996)
random_flip = yes
random_scale = (0.8, 1.25)
num_workers = 2
train_set = training
val_set = validation
coco_gt =